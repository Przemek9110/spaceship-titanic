{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spaceship Titanic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project Description**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good. The Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars. While rounding Alpha Centauri en route to its first destination—the torrid 55 Cancri E—the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension! To help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship’s damaged computer system. Help save them and change history!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Goals**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get highest classification accuracy score calculated by percentage of predicted labels for transported passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1.Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#from ydata_profiling import ProfileReport\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "import catboost as cb\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, RocCurveDisplay\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "# from sklearn.experimental import enable_iterative_imputer 'try in future'\n",
    "# from sklearn.impute import IterativeImputer\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because data size is acceptable for memory capacity we're reading entire data from csv to DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = 'data/train.csv'\n",
    "TEST_DATA_PATH = 'data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_PATH, index_col=False)\n",
    "test = pd.read_csv(TEST_DATA_PATH, index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.Data processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Field Descriptions**\n",
    "\n",
    "```PassengerId``` - A unique Id for each passenger. Each Id takes the form `gggg_pp` where `gggg` indicates a group the passenger is travelling with and `pp` is their number within the group. People in a group are often family members, but not always.\n",
    "\n",
    "```HomePlanet``` - The planet the passenger departed from, typically their planet of permanent residence.\n",
    "\n",
    "```CryoSleep``` - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n",
    "\n",
    "```Cabin``` - The cabin number where the passenger is staying. Takes the form `deck/num/side`, where `side` can be either `P` for Port or `S` for Starboard.\n",
    "\n",
    "```Destination``` - The planet the passenger will be debarking to.\n",
    "\n",
    "```Age``` - The age of the passenger.\n",
    "\n",
    "```VIP``` - Whether the passenger has paid for special VIP service during the voyage.\n",
    "\n",
    "```RoomService```, ```FoodCourt```, ```ShoppingMall```, ```Spa```, ```VRDeck``` - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n",
    "\n",
    "```Name``` - The first and last names of the passenger.\n",
    "\n",
    "```Transported``` - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Few steps before we start data analysis**\n",
    "\n",
    "As was mentioned above each ```PassengerId``` takes the form `gggg_pp`, ```Cabin``` takes the form `deck/num/side` and ```Name``` contains first and last name my first step will be creating new columns from those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.concat([train, test]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_passenger_id(df):\n",
    "    df[['Group', 'pp']] = df.PassengerId.str.split('_', expand=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cabin(df):\n",
    "    df[['Deck', 'Num', 'Side']] = df.Cabin.str.split('/', expand=True)\n",
    "    df.drop(columns='Cabin',axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_name(df):\n",
    "    df[['FirstName', 'LastName']] = df.Name.str.split(' ', expand=True)\n",
    "    df.drop(columns='Name',axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = (temp\n",
    "        .pipe(split_passenger_id)\n",
    "        .pipe(split_cabin)\n",
    "        .pipe(split_name)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Background information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_null_cells_in_row(df):\n",
    "    return df.isnull().sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of rows in train data is {temp.shape[0]}, and the number of columns is {temp.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.isna().sum().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_number_of_null_cells_in_row(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately ~25% rows include null values that must be filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['Transported'].value_counts().plot(kind='pie', autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see train dataset is almost perfectly balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1.1 Age --> Other**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(15,figsize=(18,36))\n",
    "sns.countplot(x='Age',hue='Transported',data=temp, ax=ax[14])\n",
    "sns.countplot(x='Age',hue='HomePlanet',data=temp, ax=ax[0])\n",
    "sns.countplot(x='Age',hue='CryoSleep',data=temp, ax=ax[1])\n",
    "sns.countplot(x='Age',hue='Destination',data=temp, ax=ax[2])\n",
    "sns.countplot(x='Age',hue='VIP',data=temp, ax=ax[3])\n",
    "sns.countplot(x='Age',hue='Deck',data=temp, ax=ax[4])\n",
    "sns.countplot(x='Age',hue='Side',data=temp, ax=ax[5])\n",
    "\n",
    "temp.groupby('Age')['RoomService'].sum().plot(kind='bar', ax=ax[6], legend=True)\n",
    "temp.groupby('Age')['FoodCourt'].sum().plot(kind='bar', ax=ax[7], legend=True)\n",
    "temp.groupby('Age')['ShoppingMall'].sum().plot(kind='bar', ax=ax[8], legend=True)\n",
    "temp.groupby('Age')['Spa'].sum().plot(kind='bar', ax=ax[9], legend=True)\n",
    "temp.groupby('Age')['VRDeck'].sum().plot(kind='bar', ax=ax[10], legend=True)\n",
    "temp.groupby('Age')['Group'].count().plot(kind='bar', ax=ax[11], legend=True)\n",
    "temp.groupby('Age')['pp'].count().plot(kind='bar', ax=ax[12], legend=True)\n",
    "temp.groupby('Age')['Num'].count().plot(kind='bar', ax=ax[13], legend=True)\n",
    "fig.tight_layout()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age is a continuous feature were necessery is discretization which I do grouping some ages basing on visualization above.<br>\n",
    "Groups:<br>\n",
    "0 - age equal 0 many unborns/newborns was transported it might have impact for future training. <br>\n",
    "1 - age 1-4 high transportation ratio <br>\n",
    "2 - age 5-12 low count of persons in those age group i compare to other<br>\n",
    "3 - age 13-17 the first group in which expenses appear<br>\n",
    "4 - age 18-24 up to 24 age old only 4 VIP exist<br>\n",
    "5 - age 25-65 others<br>\n",
    "6 - age 66- count of persons above 66 years old is highly decrased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_age(df):\n",
    "    df['Age'] = pd.cut(df['Age'], bins=[0,1,5,13,18,25,66,110], labels=[0,1,2,3,4,5,6], right=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1.2 HomePlanet --> other**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(15,figsize=(20,40))\n",
    "sns.countplot(x='HomePlanet',hue='Transported',data=temp, ax=ax[14])\n",
    "sns.countplot(x='HomePlanet',hue='HomePlanet',data=temp, ax=ax[0])\n",
    "sns.countplot(x='HomePlanet',hue='CryoSleep',data=temp, ax=ax[1])\n",
    "sns.countplot(x='HomePlanet',hue='Destination',data=temp, ax=ax[2])\n",
    "sns.countplot(x='HomePlanet',hue='VIP',data=temp, ax=ax[3])\n",
    "sns.countplot(x='HomePlanet',hue='Deck',data=temp, ax=ax[4])\n",
    "sns.countplot(x='HomePlanet',hue='Side',data=temp, ax=ax[5])\n",
    "\n",
    "temp.groupby('HomePlanet')['RoomService'].sum().plot(kind='bar', ax=ax[6], legend=True)\n",
    "temp.groupby('HomePlanet')['FoodCourt'].sum().plot(kind='bar', ax=ax[7], legend=True)\n",
    "temp.groupby('HomePlanet')['ShoppingMall'].sum().plot(kind='bar', ax=ax[8], legend=True)\n",
    "temp.groupby('HomePlanet')['Spa'].sum().plot(kind='bar', ax=ax[9], legend=True)\n",
    "temp.groupby('HomePlanet')['VRDeck'].sum().plot(kind='bar', ax=ax[10], legend=True)\n",
    "temp.groupby('HomePlanet')['Group'].count().plot(kind='bar', ax=ax[11], legend=True)\n",
    "temp.groupby('HomePlanet')['pp'].count().plot(kind='bar', ax=ax[12], legend=True)\n",
    "temp.groupby('HomePlanet')['Num'].count().plot(kind='bar', ax=ax[13], legend=True)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[(temp.VIP == True) & (temp.HomePlanet == 'Earth')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp[(temp.Destination == 'PSO J318.5-22') & (temp.HomePlanet == 'Europa')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.groupby('HomePlanet')['Deck'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important information which can help fill `nan` is that no one from Earth get VIP status also only passengers from Earth get 'G' Deck and from Europa Decks 'B', 'A', 'C'<br>\n",
    "\n",
    "For only 29 passengers out of about 2000 departure from Europa,the destination is PSO J318.5-22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1.3 VIP --> expenses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp['Expenses'] = temp['RoomService'] +temp['FoodCourt'] + temp['ShoppingMall'] + temp['Spa'] + temp['VRDeck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.groupby('VIP')['Expenses'].mean().plot(kind='bar', legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see on `Age` and `HomePlanet` charts, the specific bill information does not give specific results therefore it will be collected into one column.<br>\n",
    "\n",
    "`Expenses` `nan` can be filled by mean of expenses depending on VIP status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPENSES_COLUMNS = ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']\n",
    "def combine_expenses(df):\n",
    "    df['Expenses'] = 0\n",
    "    for column in EXPENSES_COLUMNS:\n",
    "        df['Expenses'] = df['Expenses'] + df[column]\n",
    "    #temp.drop(EXPENSES_COLUMNS, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1.4 CryoSleep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp.groupby('CryoSleep')['Expenses'].sum().plot(kind='bar', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[(temp.CryoSleep == True) & (temp.VIP == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passengers in `CryoSleep` have no expenses and if they travel from `Europa` are VIP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1.5 Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i, feature in enumerate(EXPENSES_COLUMNS + [\"Expenses\"]):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    sns.boxplot(x = temp[feature])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp['RoomService'].mean())\n",
    "print(temp[temp['RoomService'] > 0]['RoomService'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp['FoodCourt'].mean())\n",
    "print(temp[temp['FoodCourt'] > 0]['FoodCourt'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp['ShoppingMall'].mean())\n",
    "print(temp[temp['ShoppingMall'] > 0]['ShoppingMall'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp['Spa'].mean())\n",
    "print(temp[temp['Spa'] > 0]['Spa'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp['VRDeck'].mean())\n",
    "print(temp[temp['VRDeck'] > 0]['VRDeck'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Expenses` - due to many outliers `nan` will be filled by median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TODO Check similar LastNames for HomePlanet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(temp.corr(), annot=True, cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Data cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Describe steps based on previous analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_test = (train_test\n",
    "              .pipe(split_cabin)\n",
    "              .pipe(split_name)\n",
    "              .pipe(split_passenger_id)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test[train_test.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no duplicated rows between train and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.1 VIP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.VIP.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['VIP'] = np.where(train_test.VIP.isnull() & (train_test.Age < 25), False, train_test.VIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['VIP'] = np.where(train_test.VIP.isnull() & train_test.HomePlanet.str.contains('Earth'), False, train_test.VIP)\n",
    "train_test['VIP'] = np.where(train_test.VIP.isnull() & train_test.HomePlanet.str.contains('Europa'), True, train_test.VIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.VIP.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.2 HomePlanet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group_age after fill nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.HomePlanet.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['HomePlanet'] = np.where(train_test.HomePlanet.isnull() & train_test.Deck.str.contains('G'), 'Earth', train_test.HomePlanet)\n",
    "\n",
    "train_test['HomePlanet'] = np.where(train_test.HomePlanet.isnull() & train_test.Deck.str.contains('B'), 'Europa', train_test.HomePlanet)\n",
    "\n",
    "train_test['HomePlanet'] = np.where(train_test.HomePlanet.isnull() & train_test.Deck.str.contains('A'), 'Europa', train_test.HomePlanet)\n",
    "\n",
    "train_test['HomePlanet'] = np.where(train_test.HomePlanet.isnull() & train_test.Deck.str.contains('C'), 'Europa', train_test.HomePlanet)\n",
    "\n",
    "train_test['HomePlanet'] = np.where(train_test.HomePlanet.isnull() & train_test.VIP & train_test.Deck.str.contains('F'), 'Mars', train_test.HomePlanet)\n",
    "\n",
    "train_test['HomePlanet'] = np.where(train_test.HomePlanet.isnull() & (train_test.VIP==False), 'Earth', train_test.HomePlanet)\n",
    "\n",
    "train_test['HomePlanet'] = np.where(train_test.HomePlanet.isnull() & train_test.CryoSleep & train_test.VIP, 'Europa', train_test.HomePlanet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.HomePlanet.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still 2 `nan` where just 1 is from test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.3 Expenses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in EXPENSES_COLUMNS:\n",
    "    if_VIP_true = train_test[(train_test[column] > 0) & (train_test.VIP)][column].median()\n",
    "    if_VIP_false = train_test[(train_test[column] > 0) & (train_test.VIP == False)][column].median()\n",
    "    train_test[column] = np.where(train_test[column].isnull() & train_test.CryoSleep, 0, train_test[column])\n",
    "    train_test[column] = np.where((train_test[column].isnull() )& (train_test.Age < 13), 0, train_test[column])\n",
    "    train_test[column] = np.where(train_test[column].isnull() & (train_test.CryoSleep == False) & train_test.VIP, if_VIP_true, train_test[column])\n",
    "    train_test[column] = np.where(train_test[column].isnull() & (train_test.CryoSleep == False), if_VIP_false, train_test[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = combine_expenses(train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_test.Expenses.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.4 CryoSleep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.CryoSleep.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['CryoSleep'] = np.where(train_test.CryoSleep.isnull() & (train_test.Expenses == 0), True, train_test.CryoSleep)\n",
    "train_test['CryoSleep'] = np.where(train_test.CryoSleep.isnull() & (train_test.Expenses > 0), False, train_test.CryoSleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.CryoSleep.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.5 Age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.Age.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_without_expenses = train_test[(train_test.Age < 13)]['Age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_with_expenses = train_test[(train_test.Age > 12)]['Age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['Age'] = np.where(train_test.Age.isnull() & (train_test.Expenses == 0), median_without_expenses, train_test.Age)\n",
    "train_test['Age'] = np.where(train_test.Age.isnull() & (train_test.Expenses > 0), median_with_expenses, train_test.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.Age.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.6 Destination**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No correlation found filled using `mode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.Destination.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['Destination'] = train_test.Destination.fillna(value=train_test.Destination.mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.Destination.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ProfileReport(train, title='Spaceship Titanic').to_file('Spaceship_Titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.7 Deck/Num/Side**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.Deck.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of `nan` in `Side` and `Num` is same because was created from one feature as `Deck` <br>\n",
    "First I will try restore `Deck` values from `HomePlanet` were was 'G' for 'Earth' and 'B','A', 'C' for 'Europa' <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deck**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test[train_test.Deck.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.groupby(['HomePlanet', 'VIP', 'Deck'])['Destination'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common `Deck` for 'Mars' is 'F' that is why I decide to fill rest of `nan` by 'F'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['Deck'] = np.where(train_test.Deck.isnull() & train_test.HomePlanet.str.contains('Earth'), 'G', train_test.Deck)\n",
    "train_test['Deck'] = np.where(train_test.Deck.isnull() & train_test.HomePlanet.str.contains('Europa'), 'B', train_test.Deck)\n",
    "train_test['Deck'] = np.where(train_test.Deck.isnull() & train_test.HomePlanet.str.contains('Mars'), 'F', train_test.Deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.Deck.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Num**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Num` will be filled based on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.Num.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num` and `Side` are same when `Group` and `LastName` are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['Num'] = np.where(train_test.Num.isnull() & train_test.LastName.eq(train_test.LastName.shift()), train_test.Num.shift(), train_test.Num)\n",
    "train_test['Side'] = np.where(train_test.Side.isnull() & train_test.LastName.eq(train_test.LastName.shift()), train_test.Side.shift(), train_test.Side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_test.Num.isnull().sum())\n",
    "print(train_test.Side.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other `nan` will be made up `mode` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['Num'] = train_test.Num.fillna(value=train_test.Num.mode()[0])\n",
    "train_test['Side'] = train_test.Side.fillna(value=train_test.Side.mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.8 LastName**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['LastName'] = np.where(train_test.LastName.isnull(), train_test.LastName.shift(), train_test.LastName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.LastName.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.9 Repeat steps**\n",
    "Repetition of the steps is necessary because on the `nan` completed in the next steps were the basis for filling in the previous gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VIP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['VIP'] = np.where(train_test.VIP.isnull() & (train_test.Age < 25), False, train_test.VIP)\n",
    "train_test['VIP'] = np.where(train_test.VIP.isnull() & train_test.HomePlanet.str.contains('Earth'), False, train_test.VIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HomePlanet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['HomePlanet'] = np.where(train_test.HomePlanet.isnull() & train_test.Deck.str.contains('G'), 'Earth', train_test.HomePlanet)\n",
    "\n",
    "train_test['HomePlanet'] = np.where(train_test.HomePlanet.isnull() & train_test.Deck.str.contains('B'), 'Europa', train_test.HomePlanet)\n",
    "\n",
    "train_test['HomePlanet'] = np.where(train_test.HomePlanet.isnull() & train_test.Deck.str.contains('A'), 'Europa', train_test.HomePlanet)\n",
    "\n",
    "train_test['HomePlanet'] = np.where(train_test.HomePlanet.isnull() & train_test.Deck.str.contains('C'), 'Europa', train_test.HomePlanet)\n",
    "\n",
    "train_test['HomePlanet'] = np.where(train_test.HomePlanet.isnull() & train_test.VIP & train_test.Deck.str.contains('F'), 'Mars', train_test.HomePlanet)\n",
    "\n",
    "train_test['HomePlanet'] = np.where(train_test.HomePlanet.isnull() & (train_test.VIP==False), 'Earth', train_test.HomePlanet)\n",
    "\n",
    "train_test['HomePlanet'] = np.where(train_test.HomePlanet.isnull() & train_test.CryoSleep & train_test.VIP, 'Europa', train_test.HomePlanet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expenses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in EXPENSES_COLUMNS:\n",
    "    if_VIP_true = train_test[(train_test[column] > 0) & (train_test.VIP)][column].median()\n",
    "    if_VIP_false = train_test[(train_test[column] > 0) & (train_test.VIP == False)][column].median()\n",
    "    train_test[column] = np.where(train_test[column].isnull() & train_test.CryoSleep, 0, train_test[column])\n",
    "    train_test[column] = np.where((train_test[column].isnull() )& (train_test.Age < 13), 0, train_test[column])\n",
    "    train_test[column] = np.where(train_test[column].isnull() & (train_test.CryoSleep == False) & train_test.VIP, if_VIP_true, train_test[column])\n",
    "    train_test[column] = np.where(train_test[column].isnull() & (train_test.CryoSleep == False), if_VIP_false, train_test[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = combine_expenses(train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CryoSleep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['CryoSleep'] = np.where(train_test.CryoSleep.isnull() & (train_test.Expenses == 0), True, train_test.CryoSleep)\n",
    "train_test['CryoSleep'] = np.where(train_test.CryoSleep.isnull() & (train_test.Expenses > 0), False, train_test.CryoSleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.drop('Transported', axis=1).isna().sum().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.drop(['FirstName'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.10 Drop `nan`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_number_of_null_cells_in_row(train_test.drop('Transported', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nan` in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_number_of_null_cells_in_row(train_test[train_test.Transported.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_test[~train_test.Transported.isnull()].dropna().copy()\n",
    "test = train_test[train_test.Transported.isnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_number_of_null_cells_in_row(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1841 rows has been restored by filling `nan` in train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = group_age(train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.VIP = train_test.VIP.replace({True : 1, False : 0})\n",
    "train_test.CryoSleep = train_test.CryoSleep.replace({True : 1, False : 0})\n",
    "train_test.Side = train_test.Side.replace({'P' : 1, 'S' : 0}) \n",
    "# train_test.Transported = train_test.Transported.astype('bool') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['Age', 'LastName', 'Num','Group', 'pp']  # deleted pp\n",
    "for column in categorical:\n",
    "    encoder = LabelEncoder()\n",
    "    train_test[column] = encoder.fit_transform(train_test[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = pd.get_dummies(train_test, columns=['HomePlanet', 'Destination', 'Deck'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3.1 Data splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.reset_index(drop=True, inplace=True)\n",
    "train = train_test[~train_test.Transported.isnull()].copy().astype('float64')\n",
    "\n",
    "test = train['Transported'].copy().astype('float64')\n",
    "train = train.drop(labels=['PassengerId', 'Transported'], axis=1)\n",
    "\n",
    "submission_test = train_test[train_test.Transported.isnull()].copy()\n",
    "submission = submission_test.PassengerId.copy()\n",
    "submission_test = submission_test.drop(labels =['PassengerId', 'Transported'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train, y_test = train_test_split(train,test, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val,y_train, y_val = train_test_split(X_train,y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3.2 Data Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "# X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test[~train_test.Transported.isnull()]['Transported'].value_counts().plot(kind='pie', autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data cleaning dataset balance stay same as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithms it will use:<br>\n",
    "    XGBoost<br>\n",
    "    LightGBM<br>\n",
    "    CatBoost<br>\n",
    "    \n",
    "Neural Network:<br>\n",
    "    PyTorch logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 Machine Learning algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_mean(model, n_splits):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(train):\n",
    "\n",
    "        X_train, X_test = train.iloc[train_idx], train.iloc[test_idx]\n",
    "        y_train, y_test = test[train_idx], test[test_idx]\n",
    "\n",
    "        preds = model.predict(X_test)\n",
    "        loss = accuracy_score(y_test, preds)\n",
    "        scores.append(loss)\n",
    "    accuracy = np.mean(scores)\n",
    "    print(f\"KFold mean score: {accuracy}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_test, preds):\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    recall = recall_score(y_test, preds)\n",
    "    precision = precision_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    return accuracy, recall, precision, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1.1 XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function and evaluation metric serve two different purposes. \n",
    "#The loss function is used by the model to learn the relationship between input and output. \n",
    "#The evaluation metric is used to assess how good the learned relationship is. \n",
    "#Here is a link to a discussion of model https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    params = {\n",
    "        #Parameters to tune\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 20),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        \n",
    "        #Parameters for faster speed\n",
    "        'colsample_bytree' : trial.suggest_loguniform('colsample_bytree', 0.01, 1.0),\n",
    "        'subsample': trial.suggest_loguniform('subsample', 0.01, 1.0),\n",
    "        \n",
    "        #Parameters to control overfitting\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
    "        #'gamma': trial.suggest_loguniform('gamma',1, 10),\n",
    "        'early_stopping_rounds' : trial.suggest_int('early_stopping_rounds',5,30),\n",
    "        \n",
    "        #Regularizers for bigger dataset\n",
    "        'alpha' : trial.suggest_int('alpha', 0, 5),\n",
    "        #'lambda' : trial.suggest_int('lambda', 1, 5),\n",
    "        \n",
    "        #Loss function and evaluation metric\n",
    "        'objective' : 'binary:logistic', # represents cross entropy loss function\n",
    "        'eval_metric': 'logloss', #according to objective when used with binary classification objective should be 'binary:logistic'\n",
    "        \n",
    "        #'tree_method' : 'gpu_hist'\n",
    "    \n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Fit the model\n",
    "    optuna_model = xgb.XGBClassifier(**params)\n",
    "    optuna_model.fit(X_train, y_train, verbose=False, eval_set=[(X_val, y_val)])\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = optuna_model.predict(X_test)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = xgb.XGBClassifier(**study.best_trial.params)\n",
    "xgbc.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False) \n",
    "kfold_acc = kfold_mean(xgbc,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, recall, precision, f1 = calculate_metrics(y_test, xgbc.predict(X_test))\n",
    "\n",
    "xgbc_metrics = {'Model': 'XGBoost','KFold cv accuracy': kfold_acc ,'Accuracy': accuracy, 'recall': recall, 'precision': precision, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1.2 LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    params={\n",
    "    #Parameters to tune\n",
    "    'num_leaves' : trial.suggest_int('num_leaves',2,800), #should be less than 2^max_depth lower better acc\n",
    "    'max_depth' : trial.suggest_int('max_depth', 1, 15),\n",
    "    'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 0, 400),\n",
    "    \n",
    "    #Parameters for better accurancy\n",
    "    'max_bin' : trial.suggest_int('max_bin',100,600), #small number of bins may reduce training accuracy \n",
    "                                                    #but may increase general power (deal with over-fitting)\n",
    "    'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.5),\n",
    "    \n",
    "    #Parameters to control over-fitting\n",
    "    'min_gain_to_split' : trial.suggest_loguniform('min_gain_to_split', 1.0, 10.0),\n",
    "    'early_stopping' : trial.suggest_int('early_stopping_rounds',5,30),\n",
    "    \n",
    "    #Loss function and evaluation metric\n",
    "    'objective' : 'binary', \n",
    "    'metric': 'binary_logloss', \n",
    "        \n",
    "    #Regularizers\n",
    "    'lambda_l1' : trial.suggest_loguniform('lambda_l1', 0.01, 5.0),\n",
    "    #'lambda_l2' : trial.suggest_loguniform('lambda_l2', 0.01, 5.0),\n",
    "        \n",
    "    'device_type' : 'CPU',\n",
    "    'n_jobs' : -1,\n",
    "    'verbose' : -1,\n",
    "    'verbose_eval' : -1\n",
    "}\n",
    "    \n",
    "    \n",
    "    # Fit the model\n",
    "    optuna_model = lgbm.LGBMClassifier(**params)\n",
    "    optuna_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = optuna_model.predict(X_test)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = study.best_trial\n",
    "lgbmc = lgbm.LGBMClassifier(**trial.params)\n",
    "lgbmc.fit(X_train, y_train, eval_set=[(X_val, y_val)],verbose=False,)\n",
    "kfold_acc = kfold_mean(lgbmc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, recall, precision, f1 = calculate_metrics(y_test, lgbmc.predict(X_test))\n",
    "\n",
    "lgbmc_metrics = {'Model': 'LightGBM','KFold cv accuracy': kfold_acc ,'Accuracy': accuracy, 'recall': recall, 'precision': precision, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbmc_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_pred_lgbmc = lgbmc.predict(submission_test)\n",
    "\n",
    "# sub_pred_lgbmc = sub_pred_lgbmc.astype('bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1.2 CatBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "# https://practicaldatascience.co.uk/machine-learning/how-to-tune-a-catboostclassifier-model-with-optuna\n",
    "    params={\n",
    "    #Parameters to tune\n",
    "    'iterations' : trial.suggest_int('iterations',100,2000),\n",
    "    'depth' : trial.suggest_int('depth', 1, 15),    \n",
    "    'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 0, 200),\n",
    "    \n",
    "    #Parameters to control overfitting\n",
    "    'early_stopping_rounds': trial.suggest_int('early_stopping_rounds',5,30),\n",
    "    'od_type' : trial.suggest_categorical(\"od_type\", [\"IncToDec\", \"Iter\"]),\n",
    "    'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
    "        \n",
    "    #Regularization\n",
    "    'l2_leaf_reg' : trial.suggest_float(\"l2_leaf_reg\", 1e-8, 100.0),\n",
    "        \n",
    "    #loss function and evaluation metric\n",
    "    'objective' : trial.suggest_categorical(\"objective\", ['Logloss']),\n",
    "    'eval_metric' : trial.suggest_categorical('eval_metric',['Accuracy']),\n",
    "    \n",
    "    #'task_type' : 'GPU,'\n",
    "    'verbose' : False\n",
    "}\n",
    "    \n",
    "    \n",
    "    # Fit the model\n",
    "    optuna_model = cb.CatBoostClassifier(**params)\n",
    "    optuna_model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = optuna_model.predict(X_test)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc = cb.CatBoostClassifier(**study.best_trial.params)\n",
    "cbc.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "kfold_acc = kfold_mean(cbc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy, recall, precision, f1 = calculate_metrics(y_test, cbc.predict(X_test))\n",
    "\n",
    "cbc_metrics = {'Model': 'CatBoost','KFold cv accuracy': kfold_acc ,'Accuracy': accuracy, 'recall': recall, 'precision': precision, 'f1': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Models compare**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame([xgbc_metrics, lgbmc_metrics, cbc_metrics]).set_index('Model', drop=True)\n",
    "(metrics.style\n",
    "    .highlight_min('f1', color='red')\n",
    "    .highlight_max('f1', color='lightgreen')\n",
    "    .highlight_min('precision', color='red')\n",
    "    .highlight_max('precision', color='lightgreen')\n",
    "    .highlight_min('recall', color='red')\n",
    "    .highlight_max('recall', color='lightgreen')\n",
    "    .highlight_min('Accuracy', color='red')\n",
    "    .highlight_max('Accuracy', color='lightgreen')\n",
    "    .highlight_min('KFold cv accuracy', color='red')\n",
    "    .highlight_max('KFold cv accuracy', color='lightgreen')\n",
    "     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "RocCurveDisplay.from_estimator(lgbmc, X_test, y_test, ax=ax)\n",
    "RocCurveDisplay.from_estimator(xgbc, X_test, y_test, ax=ax)\n",
    "RocCurveDisplay.from_estimator(cbc, X_test, y_test, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred_cbc = cbc.predict(submission_test)\n",
    "\n",
    "sub_pred_cbc = sub_pred_cbc.astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "sub['Transported'] = sub_pred_cbc\n",
    "\n",
    "sub.to_csv('data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusions**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "#     else \"mps\"\n",
    "#     if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs\n",
    "        \n",
    "#Create model\n",
    "model = LogisticRegression(X_train.shape[1], 1).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold parameters\n",
    "num_folds = 10\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Cross-Validation\n",
    "scores = []\n",
    "for train_index, test_index in kf.split(train):\n",
    "    X_train, X_test = torch.Tensor(train.iloc[train_index,:].values), torch.Tensor(train.iloc[test_index,:].values)\n",
    "    y_train, y_test = torch.Tensor(test[train_index].values), torch.Tensor(test[test_index].values)\n",
    "\n",
    " # Model Training\n",
    "    losses = []\n",
    "    losses_test = []\n",
    "    Iterations = []\n",
    "    iter = 0\n",
    "    epochs = 20000\n",
    "    for epoch in range(int(epochs)):\n",
    "        x =  X_train.to(device)\n",
    "        labels =  y_train.to(device)\n",
    "        optimizer.zero_grad() # Setting our stored gradients equal to zero\n",
    "        outputs = model(x)\n",
    "        loss = criterion(torch.squeeze(outputs), labels) \n",
    "\n",
    "        loss.backward() # Computes the gradient of the given tensor w.r.t. the weights/bias\n",
    "        optimizer.step() # Updates weights and biases with the optimizer (SGD)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Calculating the loss and accuracy for the test dataset\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        outputs_test = torch.squeeze(model(X_test.to(device)))\n",
    "        predicted_test = outputs_test.cpu().round().detach().numpy()\n",
    "        total_test += y_test.size(0)\n",
    "        correct_test += np.sum(predicted_test == y_test.detach().numpy())\n",
    "        accuracy_test = 100 * correct_test/total_test\n",
    "        scores.append(accuracy_test)\n",
    "    accuracy = np.mean(scores)\n",
    "    \n",
    "print(f\"{device} KFold mean accuracy score: {accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
